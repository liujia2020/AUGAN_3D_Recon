import sys
import os
import torch
import shutil

# --- è·¯å¾„ Hack ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.augan_model import AuganModel

def run_test():
    print("ğŸš€ å¯åŠ¨ AuganModel é›†æˆæµ‹è¯• (Integration Test)...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   è®¾å¤‡: {device}")

    # --- 1. æ¨¡æ‹Ÿå®Œæ•´çš„é…ç½®å‚æ•° ---
    class MockOpt:
        # [åŸºç¡€]
        # ä¿®æ­£: å¿…é¡»æ˜¯åˆ—è¡¨ [0]ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸² '0'
        gpu_ids = [0] if torch.cuda.is_available() else []
        isTrain = True
        checkpoints_dir = './tests/temp_checkpoints'
        name = 'integration_test_run'
        model = 'augan'
        verbose = False
        suffix = ''
        
        # [ç½‘ç»œç»“æ„]
        input_nc = 1
        output_nc = 1
        ngf = 64
        ndf = 64
        netG = 'unet_3d'
        netD = 'pixel' 
        norm = 'instance'
        init_type = 'normal'
        init_gain = 0.02
        no_dropout = False
        
        # [è®­ç»ƒä¸ Loss]
        gan_mode = 'vanilla'
        lr = 0.0002
        beta1 = 0.5
        lr_d_ratio = 1.0
        lambda_L2 = 100.0
        
        # [!!] æ–°å¢ä¿®å¤: è¡¥é½ Scheduler ç¼ºå°‘çš„å‚æ•°
        lr_policy = 'linear'
        epoch_count = 1        # èµ·å§‹ epoch
        n_epochs = 100         #ä»¥æ­¤å­¦ä¹ ç‡è®­ç»ƒå¤šå°‘ epoch
        n_epochs_decay = 100   # è¡°å‡ epoch æ•°
        lr_decay_iters = 50    # å¦‚æœç”¨ step ç­–ç•¥éœ€è¦çš„å‚æ•°
        continue_train = False # æ˜¯å¦ç»§ç»­è®­ç»ƒ
        load_iter = 0          # åŠ è½½è¿­ä»£æ¬¡æ•°

        # [ç‰©ç†å‚æ•°]
        patch_size_d = 256
        patch_size_h = 64
        patch_size_w = 64
        batch_size = 2 

    opt = MockOpt()
    
    # æ¸…ç†æ—§çš„æµ‹è¯•æ–‡ä»¶
    if os.path.exists(opt.checkpoints_dir):
        shutil.rmtree(opt.checkpoints_dir)

    # --- 2. åˆå§‹åŒ–æ¨¡å‹ ---
    try:
        model = AuganModel(opt)
        model.setup(opt) # è¿™é‡Œä¼šè°ƒç”¨ get_schedulerï¼Œç°åœ¨å‚æ•°é½äº†åº”è¯¥èƒ½è¿‡
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (G + D + Optimizers + Schedulers)ã€‚")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- 3. æ„é€ ä¼ªæ•°æ® (Dummy Batch) ---
    print("\nğŸ“¦ æ„é€ ä¼ªæ•°æ®...")
    input_shape = (opt.batch_size, opt.input_nc, opt.patch_size_d, opt.patch_size_h, opt.patch_size_w)
    
    dummy_lq = torch.randn(*input_shape)
    dummy_hq = torch.randn(*input_shape)
    
    # å½’ä¸€åŒ–æ¨¡æ‹Ÿ
    dummy_lq = torch.clamp(dummy_lq, -1, 1)
    dummy_hq = torch.clamp(dummy_hq, -1, 1)
    
    data = {
        'LQ': dummy_lq,
        'HQ': dummy_hq,
        'lq_path': ['fake_path_1.nii', 'fake_path_2.nii']
    }
    
    # --- 4. è¿è¡Œå•æ­¥ä¼˜åŒ– ---
    print("ğŸ”„ æ‰§è¡Œ optimize_parameters() ...")
    try:
        model.set_input(data)
        model.optimize_parameters()
        print("âœ… ä¼˜åŒ–æ­¥æ‰§è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–æ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- 5. æ£€æŸ¥ Loss ---
    print("\nğŸ“Š æ£€æŸ¥æŸå¤±å€¼ (Loss Check):")
    losses = model.get_current_losses()
    
    all_good = True
    for name, value in losses.items():
        print(f"   -> {name}: {value:.4f}")
        if value == 0.0:
            print(f"      âš ï¸ è­¦å‘Š: Loss ä¸º 0ï¼Œå¯èƒ½æ¢¯åº¦æ–­è£‚ï¼Ÿ")
        if torch.isnan(torch.tensor(value)):
            print(f"      âŒ é”™è¯¯: Loss ä¸º NaN (æ¢¯åº¦çˆ†ç‚¸)ï¼")
            all_good = False
            
    if all_good:
        print("âœ… æ‰€æœ‰ Loss æ•°å€¼æ­£å¸¸ã€‚")
    
    # --- 6. æ£€æŸ¥è¾“å‡ºå½¢çŠ¶ ---
    if hasattr(model, 'fake_hq'):
        print(f"\nğŸ–¼ï¸  ç”Ÿæˆç»“æœå½¢çŠ¶: {model.fake_hq.shape}")
        if list(model.fake_hq.shape) == list(input_shape):
            print("âœ… è¾“å‡ºå½¢çŠ¶åŒ¹é…ã€‚")
        else:
            print("âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼")
    
    print("\nğŸ‰ é›†æˆæµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡ŒçœŸå®è®­ç»ƒã€‚")

if __name__ == '__main__':
    run_test()